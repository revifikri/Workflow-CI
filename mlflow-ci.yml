name: MLflow Model Training CI

on:
  push:
    branches: [main, develop]
    paths: 
      - 'MLProject/modelling.py'
      - 'MLProject/conda.yaml'
      - 'MLProject/data_processed.csv'
      - 'MLProject/MLProject'
  schedule:
    - cron: '0 2 * * 1'  # Weekly retraining
  workflow_dispatch:  # Manual trigger

env:
  MLFLOW_TRACKING_URI: "http://localhost:5000"
  EXPERIMENT_NAME: "CI_Pipeline_Experiment"

jobs:
  model_training:
    runs-on: windows-latest  # ubuntu or windows-latest for Windows
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12.9'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install mlflow>=2.0.0 pandas numpy scikit-learn xgboost joblib
      
      - name: Validate MLProject Structure
        run: |
          test -d MLProject || (echo "❌ MLProject folder not found" && exit 1)
          test -f MLProject/MLProject || (echo "❌ MLProject configuration file not found" && exit 1)
          test -f MLProject/modelling.py || (echo "❌ modelling.py not found" && exit 1)
          test -f MLProject/conda.yaml || (echo "❌ conda.yaml not found" && exit 1)
          test -f MLProject/data_processed.csv || (echo "❌ data_processed.csv not found" && exit 1)
          echo "✅ All required files found"
      
      - name: Start MLflow Server
        run: |
          mlflow server --host 127.0.0.1 --port 1234 --backend-store-uri sqlite:///mlflow.db &
          sleep 15
          curl -f http://127.0.0.1:1234/health
      
      - name: Run MLflow Project Training
        run: |
          cd MLProject
          mlflow run . -P data_path=data_processed.csv -P experiment_name=$EXPERIMENT_NAME --env-manager=local
          cd ..
      
      - name: Validate Model Quality
        run: |
          python -c "
          import mlflow, sys
          mlflow.set_tracking_uri('$MLFLOW_TRACKING_URI')
          exp = mlflow.get_experiment_by_name('$EXPERIMENT_NAME')
          runs = mlflow.search_runs(experiment_ids=[exp.experiment_id])
          best_r2 = runs['metrics.test_r2'].max()
          assert best_r2 > 0.7, f'Quality failed: {best_r2}'
          print(f'✅ Quality passed: {best_r2:.4f}')
          "
      
      - name: Test Serving Compatibility
        run: |
          python mlflow_serve.py info